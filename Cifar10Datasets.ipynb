{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b_itR16PduV9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformasi dataset dengan beberapa langkah preprocessing\n",
        "transform = transforms.Compose([\n",
        "    # Mengubah gambar menjadi format tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalisasi nilai pixel gambar ke rentang [-1, 1]\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "xszpd7t3d8rG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 Dataset\n",
        "batch_size = 32\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pnt9iyFd_8K",
        "outputId": "79aac71b-4f1a-4f80-d06d-3f49d29bef9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membagi dataset pelatihan menjadi subset pelatihan dan validasi\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% data untuk pelatihan\n",
        "val_size = len(train_dataset) - train_size  # Sisanya 20% untuk validasi\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])  # Membagi dataset\n",
        "\n",
        "# Membuat DataLoader untuk subset pelatihan, validasi, dan pengujian\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # DataLoader untuk pelatihan\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)     # DataLoader untuk validasi\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # DataLoader untuk pengujian"
      ],
      "metadata": {
        "id": "0HAk4GIreG2I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Arsitektur\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, kernel_size=3, pooling_type='max'):\n",
        "        super(CNN, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, padding=padding)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        if pooling_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pooling_type == 'avg':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZuiOQYcKeKQS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk Melatih dan Mengevaluasi Model\n",
        "def train_and_evaluate(kernel_size, pooling_type, optimizer_type, epochs, early_stopping_patience=10):\n",
        "    # Menentukan perangkat (GPU atau CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Inisialisasi model CNN dengan parameter kernel_size dan pooling_type\n",
        "    model = CNN(kernel_size=kernel_size, pooling_type=pooling_type).to(device)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Pemilihan optimizer berdasarkan jenis yang ditentukan\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Scheduler untuk menurunkan learning rate jika tidak ada perbaikan\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
        "\n",
        "    # Variabel untuk melacak loss terbaik dan jumlah epoch tanpa peningkatan\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Evaluating with params: {{'epochs': {epochs}, 'kernel_size': {kernel_size}, 'optimizer_type': '{optimizer_type}', 'pooling_type': '{pooling_type}'}}\")\n",
        "\n",
        "    # Loop untuk setiap epoch\n",
        "    for epoch in range(epochs):\n",
        "        # Mode pelatihan\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Reset gradien, lakukan forward dan backward pass, serta update parameter\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Rata-rata loss untuk data pelatihan\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Mode evaluasi untuk validasi\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Menampilkan hasil pada setiap epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Update scheduler dengan validasi loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early Stopping jika tidak ada perbaikan pada validasi loss\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Evaluasi akurasi pada dataset pengujian\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Menghitung akurasi\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Melakukan eksperimen dengan berbagai kombinasi parameter\n",
        "kernel_sizes = [3, 5, 7]         # Daftar ukuran kernel\n",
        "pooling_types = ['max', 'avg']   # Jenis pooling\n",
        "optimizers = ['SGD', 'RMSProp', 'Adam']  # Jenis optimizer\n",
        "epoch_list = [5, 50, 100, 250, 350]  # Daftar jumlah epoch\n",
        "\n",
        "results = []  # Untuk menyimpan hasil eksperimen\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    for pooling_type in pooling_types:\n",
        "        for optimizer in optimizers:\n",
        "            for epochs in epoch_list:\n",
        "                print(f\"Running experiment with kernel_size={kernel_size}, pooling_type={pooling_type}, optimizer={optimizer}, epochs={epochs}\")\n",
        "                accuracy = train_and_evaluate(kernel_size, pooling_type, optimizer, epochs, early_stopping_patience=10)\n",
        "                results.append((kernel_size, pooling_type, optimizer, epochs, accuracy))\n",
        "\n",
        "# Menampilkan hasil akhir eksperimen\n",
        "print(\"\\nHasil Akhir Eksperimen:\")\n",
        "for result in results:\n",
        "    print(f\"Kernel Size: {result[0]}, Pooling: {result[1]}, Optimizer: {result[2]}, Epochs: {result[3]}, Accuracy: {result[4]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWGJL4P0eZre",
        "outputId": "71981516-c7a8-4eb9-b474-228deb6cc441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 1.6387, Val Loss: 1.2828\n",
            "Epoch 2/5, Loss: 1.2530, Val Loss: 1.0405\n",
            "Epoch 3/5, Loss: 1.0911, Val Loss: 1.0781\n",
            "Epoch 4/5, Loss: 0.9806, Val Loss: 0.9340\n",
            "Epoch 5/5, Loss: 0.8986, Val Loss: 0.9010\n",
            "Accuracy: 68.89%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 1.6443, Val Loss: 1.3740\n",
            "Epoch 2/50, Loss: 1.2381, Val Loss: 1.0460\n",
            "Epoch 3/50, Loss: 1.0819, Val Loss: 0.9470\n",
            "Epoch 4/50, Loss: 0.9686, Val Loss: 0.9245\n",
            "Epoch 5/50, Loss: 0.8884, Val Loss: 0.9631\n",
            "Epoch 6/50, Loss: 0.8291, Val Loss: 0.9018\n",
            "Epoch 7/50, Loss: 0.7569, Val Loss: 0.8820\n",
            "Epoch 8/50, Loss: 0.7099, Val Loss: 0.8678\n",
            "Epoch 9/50, Loss: 0.6610, Val Loss: 0.8753\n",
            "Epoch 10/50, Loss: 0.6259, Val Loss: 0.9181\n",
            "Epoch 11/50, Loss: 0.5891, Val Loss: 0.9086\n",
            "Epoch 12/50, Loss: 0.5659, Val Loss: 0.9750\n",
            "Epoch 13/50, Loss: 0.5393, Val Loss: 0.9788\n",
            "Epoch 14/50, Loss: 0.5018, Val Loss: 0.9831\n",
            "Epoch 15/50, Loss: 0.3622, Val Loss: 1.0044\n",
            "Epoch 16/50, Loss: 0.3148, Val Loss: 1.0307\n",
            "Epoch 17/50, Loss: 0.2843, Val Loss: 1.0829\n",
            "Epoch 18/50, Loss: 0.2608, Val Loss: 1.0921\n",
            "Early stopping triggered.\n",
            "Accuracy: 72.97%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 1.6329, Val Loss: 1.2359\n",
            "Epoch 2/100, Loss: 1.2431, Val Loss: 1.0992\n",
            "Epoch 3/100, Loss: 1.0768, Val Loss: 0.9656\n",
            "Epoch 4/100, Loss: 0.9788, Val Loss: 1.0430\n",
            "Epoch 5/100, Loss: 0.8922, Val Loss: 0.9483\n",
            "Epoch 6/100, Loss: 0.8297, Val Loss: 0.8906\n",
            "Epoch 7/100, Loss: 0.7597, Val Loss: 0.8713\n",
            "Epoch 8/100, Loss: 0.7161, Val Loss: 0.8713\n",
            "Epoch 9/100, Loss: 0.6629, Val Loss: 0.9157\n",
            "Epoch 10/100, Loss: 0.6258, Val Loss: 0.9105\n",
            "Epoch 11/100, Loss: 0.5858, Val Loss: 0.9302\n",
            "Epoch 12/100, Loss: 0.5514, Val Loss: 0.9408\n",
            "Epoch 13/100, Loss: 0.5260, Val Loss: 0.9769\n",
            "Epoch 14/100, Loss: 0.3766, Val Loss: 1.0407\n",
            "Epoch 15/100, Loss: 0.3253, Val Loss: 1.0444\n",
            "Epoch 16/100, Loss: 0.3007, Val Loss: 1.0860\n",
            "Epoch 17/100, Loss: 0.2732, Val Loss: 1.1022\n",
            "Early stopping triggered.\n",
            "Accuracy: 72.62%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 1.6414, Val Loss: 1.2786\n",
            "Epoch 2/250, Loss: 1.2570, Val Loss: 1.1054\n",
            "Epoch 3/250, Loss: 1.0949, Val Loss: 0.9512\n",
            "Epoch 4/250, Loss: 0.9840, Val Loss: 0.9134\n",
            "Epoch 5/250, Loss: 0.8902, Val Loss: 0.8749\n",
            "Epoch 6/250, Loss: 0.8220, Val Loss: 0.8667\n",
            "Epoch 7/250, Loss: 0.7544, Val Loss: 0.8882\n",
            "Epoch 8/250, Loss: 0.7032, Val Loss: 0.8618\n",
            "Epoch 9/250, Loss: 0.6469, Val Loss: 0.8816\n",
            "Epoch 10/250, Loss: 0.6112, Val Loss: 0.8481\n",
            "Epoch 11/250, Loss: 0.5698, Val Loss: 0.9035\n",
            "Epoch 12/250, Loss: 0.5460, Val Loss: 0.9265\n",
            "Epoch 13/250, Loss: 0.5186, Val Loss: 0.9382\n",
            "Epoch 14/250, Loss: 0.4953, Val Loss: 1.0362\n",
            "Epoch 15/250, Loss: 0.4747, Val Loss: 1.0790\n",
            "Epoch 16/250, Loss: 0.4636, Val Loss: 1.0714\n",
            "Epoch 17/250, Loss: 0.3233, Val Loss: 1.0185\n",
            "Epoch 18/250, Loss: 0.2686, Val Loss: 1.1145\n",
            "Epoch 19/250, Loss: 0.2457, Val Loss: 1.1700\n",
            "Epoch 20/250, Loss: 0.2310, Val Loss: 1.1866\n",
            "Early stopping triggered.\n",
            "Accuracy: 72.97%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 1.6303, Val Loss: 1.2619\n",
            "Epoch 2/350, Loss: 1.2399, Val Loss: 1.0374\n",
            "Epoch 3/350, Loss: 1.0696, Val Loss: 0.9598\n",
            "Epoch 4/350, Loss: 0.9705, Val Loss: 0.9383\n",
            "Epoch 5/350, Loss: 0.8812, Val Loss: 0.8957\n",
            "Epoch 6/350, Loss: 0.8142, Val Loss: 0.8460\n",
            "Epoch 7/350, Loss: 0.7514, Val Loss: 0.8791\n",
            "Epoch 8/350, Loss: 0.6981, Val Loss: 0.9181\n",
            "Epoch 9/350, Loss: 0.6506, Val Loss: 0.8724\n",
            "Epoch 10/350, Loss: 0.6076, Val Loss: 0.9047\n",
            "Epoch 11/350, Loss: 0.5725, Val Loss: 0.9497\n",
            "Epoch 12/350, Loss: 0.5336, Val Loss: 0.9815\n",
            "Epoch 13/350, Loss: 0.3865, Val Loss: 0.9324\n",
            "Epoch 14/350, Loss: 0.3451, Val Loss: 1.0022\n",
            "Epoch 15/350, Loss: 0.3047, Val Loss: 1.0164\n",
            "Epoch 16/350, Loss: 0.2768, Val Loss: 1.0871\n",
            "Early stopping triggered.\n",
            "Accuracy: 72.75%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/5, Loss: 4.0638, Val Loss: 1.8579\n",
            "Epoch 2/5, Loss: 1.9371, Val Loss: 1.7532\n",
            "Epoch 3/5, Loss: 1.8600, Val Loss: 1.6754\n",
            "Epoch 4/5, Loss: 1.8194, Val Loss: 1.7543\n",
            "Epoch 5/5, Loss: 1.8504, Val Loss: 1.6878\n",
            "Accuracy: 37.57%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 3.7823, Val Loss: 1.8515\n",
            "Epoch 2/50, Loss: 1.9722, Val Loss: 1.8397\n",
            "Epoch 3/50, Loss: 1.8598, Val Loss: 1.7716\n",
            "Epoch 4/50, Loss: 1.8413, Val Loss: 1.6441\n",
            "Epoch 5/50, Loss: 1.7817, Val Loss: 1.6518\n",
            "Epoch 6/50, Loss: 1.7562, Val Loss: 1.6342\n",
            "Epoch 7/50, Loss: 1.7615, Val Loss: 1.5867\n",
            "Epoch 8/50, Loss: 1.7302, Val Loss: 1.5781\n",
            "Epoch 9/50, Loss: 1.7592, Val Loss: 1.5951\n",
            "Epoch 10/50, Loss: 1.7292, Val Loss: 1.5715\n",
            "Epoch 11/50, Loss: 1.7539, Val Loss: 1.5794\n",
            "Epoch 12/50, Loss: 1.8282, Val Loss: 1.5686\n",
            "Epoch 13/50, Loss: 1.7304, Val Loss: 1.6178\n",
            "Epoch 14/50, Loss: 1.7299, Val Loss: 1.6315\n",
            "Epoch 15/50, Loss: 1.7302, Val Loss: 1.6384\n",
            "Epoch 16/50, Loss: 1.7260, Val Loss: 1.6369\n",
            "Epoch 17/50, Loss: 1.7293, Val Loss: 1.6405\n",
            "Epoch 18/50, Loss: 1.7373, Val Loss: 1.6878\n",
            "Epoch 19/50, Loss: 1.5989, Val Loss: 1.4871\n",
            "Epoch 20/50, Loss: 1.5793, Val Loss: 1.4971\n",
            "Epoch 21/50, Loss: 1.5631, Val Loss: 1.5107\n",
            "Epoch 22/50, Loss: 1.5573, Val Loss: 1.4723\n",
            "Epoch 23/50, Loss: 1.5473, Val Loss: 1.4766\n"
          ]
        }
      ]
    }
  ]
}