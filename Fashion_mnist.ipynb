{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s5v7vDO7eixL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformasi Dataset\n",
        "# Transformasi dataset dengan langkah-langkah berikut:\n",
        "transform = transforms.Compose([\n",
        "    # Mengubah gambar menjadi format tensor (mengubah gambar menjadi array multidimensi PyTorch)\n",
        "    transforms.ToTensor(),\n",
        "    # Normalisasi nilai piksel ke rentang [-1, 1] dengan mean 0.5 dan standar deviasi 0.5\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "VLV7QSFZe1Z9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fashion MNIST Dataset\n",
        "batch_size = 32\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2si2SAVe5pj",
        "outputId": "5403b9e1-446d-4dd1-9e27-77fbd83bc72c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 15.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 272kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.04MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membagi dataset pelatihan menjadi subset pelatihan dan validasi\n",
        "train_size = int(0.8 * len(train_dataset))  # Mengambil 80% data untuk pelatihan\n",
        "val_size = len(train_dataset) - train_size  # Sisanya (20%) digunakan untuk validasi\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])  # Membagi dataset pelatihan\n",
        "\n",
        "# Membuat DataLoader untuk subset pelatihan, validasi, dan pengujian\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # Loader untuk data pelatihan\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)     # Loader untuk data validasi\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # Loader untuk data pengujian"
      ],
      "metadata": {
        "id": "l33dc4UKfA8s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi Arsitektur CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, kernel_size=3, pooling_type='max'):\n",
        "        super(CNN, self).__init__()\n",
        "        # Padding untuk mempertahankan ukuran spatial pada output convolusi\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        # Lapisan convolusi pertama (input: 1 channel, output: 32 channel)\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=kernel_size, padding=padding)  # Input grayscale (1 channel)\n",
        "\n",
        "        # Lapisan convolusi kedua (input: 32 channel, output: 64 channel)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        # Pooling layer, dapat berupa MaxPooling atau AveragePooling tergantung parameter\n",
        "        if pooling_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Mengambil nilai maksimum\n",
        "        elif pooling_type == 'avg':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)  # Mengambil nilai rata-rata\n",
        "\n",
        "        # Fully connected layer pertama (menghubungkan feature map ke 128 unit)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Ukuran feature map setelah pooling adalah 7x7\n",
        "\n",
        "        # Fully connected layer kedua (menghubungkan ke 10 output untuk 10 kelas)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "        # Fungsi aktivasi ReLU\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dropout untuk mencegah overfitting\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass melalui lapisan convolusi pertama, ReLU, dan pooling\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "\n",
        "        # Forward pass melalui lapisan convolusi kedua, ReLU, dan pooling\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "\n",
        "        # Meratakan output menjadi vektor 1 dimensi\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "\n",
        "        # Forward pass melalui fully connected layer pertama dan ReLU\n",
        "        x = self.relu(self.fc1(x))\n",
        "\n",
        "        # Dropout untuk regularisasi\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Forward pass melalui fully connected layer kedua untuk output prediksi\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fkOz14WdfDqn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dan Evaluasi\n",
        "def train_and_evaluate(kernel_size, pooling_type, optimizer_type, epochs, early_stopping_patience=10):\n",
        "    # Tentukan perangkat (GPU jika tersedia, jika tidak gunakan CPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Inisialisasi model dengan parameter kernel_size dan pooling_type\n",
        "    model = CNN(kernel_size=kernel_size, pooling_type=pooling_type).to(device)\n",
        "\n",
        "    # Loss function (CrossEntropyLoss untuk tugas klasifikasi multi-kelas)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Inisialisasi optimizer berdasarkan pilihan optimizer_type\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Scheduler untuk menyesuaikan learning rate secara dinamis\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
        "\n",
        "    # Variabel untuk mencatat performa terbaik\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Menampilkan konfigurasi eksperimen\n",
        "    print(f\"Evaluating with params: {{'epochs': {epochs}, 'kernel_size': {kernel_size}, 'optimizer_type': '{optimizer_type}', 'pooling_type': '{pooling_type}'}}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # === Training Step ===\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            # Transfer data ke perangkat (GPU/CPU)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Reset gradien\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Hitung loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass dan update parameter\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # === Validation Step ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Update learning rate berdasarkan scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early Stopping jika performa tidak membaik\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # === Evaluasi Akurasi pada Test Dataset ===\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Prediksi label\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Eksperimen dengan Semua Kombinasi Parameter\n",
        "kernel_sizes = [3, 5, 7]  # Ukuran kernel untuk lapisan convolusi\n",
        "pooling_types = ['max', 'avg']  # Jenis pooling (MaxPooling atau AvgPooling)\n",
        "optimizers = ['SGD', 'RMSProp', 'Adam']  # Optimizer yang akan diuji\n",
        "epoch_list = [5, 50, 100, 250, 350]  # Jumlah epoch untuk pelatihan\n",
        "\n",
        "results = []\n",
        "\n",
        "# Kombinasi semua parameter\n",
        "for kernel_size in kernel_sizes:\n",
        "    for pooling_type in pooling_types:\n",
        "        for optimizer in optimizers:\n",
        "            for epochs in epoch_list:\n",
        "                print(f\"Running experiment with kernel_size={kernel_size}, pooling_type={pooling_type}, optimizer={optimizer}, epochs={epochs}\")\n",
        "                accuracy = train_and_evaluate(kernel_size, pooling_type, optimizer, epochs, early_stopping_patience=10)\n",
        "                results.append((kernel_size, pooling_type, optimizer, epochs, accuracy))\n",
        "\n",
        "# Menampilkan Hasil Akhir\n",
        "print(\"\\nHasil Akhir Eksperimen:\")\n",
        "for result in results:\n",
        "    print(f\"Kernel Size: {result[0]}, Pooling: {result[1]}, Optimizer: {result[2]}, Epochs: {result[3]}, Accuracy: {result[4]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxC_AJX_fPVG",
        "outputId": "9dddfe54-1d82-474c-dae6-706cab0aa729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6098, Val Loss: 0.3462\n",
            "Epoch 2/5, Loss: 0.3773, Val Loss: 0.2912\n",
            "Epoch 3/5, Loss: 0.3225, Val Loss: 0.2840\n",
            "Epoch 4/5, Loss: 0.2890, Val Loss: 0.2626\n",
            "Epoch 5/5, Loss: 0.2602, Val Loss: 0.2497\n",
            "Accuracy: 90.43%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 0.6123, Val Loss: 0.3467\n",
            "Epoch 2/50, Loss: 0.3785, Val Loss: 0.2991\n",
            "Epoch 3/50, Loss: 0.3252, Val Loss: 0.2670\n",
            "Epoch 4/50, Loss: 0.2912, Val Loss: 0.2619\n",
            "Epoch 5/50, Loss: 0.2636, Val Loss: 0.2399\n",
            "Epoch 6/50, Loss: 0.2481, Val Loss: 0.2353\n",
            "Epoch 7/50, Loss: 0.2297, Val Loss: 0.2343\n",
            "Epoch 8/50, Loss: 0.2159, Val Loss: 0.2380\n",
            "Epoch 9/50, Loss: 0.2015, Val Loss: 0.2326\n",
            "Epoch 10/50, Loss: 0.1907, Val Loss: 0.2225\n",
            "Epoch 11/50, Loss: 0.1796, Val Loss: 0.2292\n",
            "Epoch 12/50, Loss: 0.1690, Val Loss: 0.2401\n",
            "Epoch 13/50, Loss: 0.1593, Val Loss: 0.2261\n",
            "Epoch 14/50, Loss: 0.1563, Val Loss: 0.2358\n",
            "Epoch 15/50, Loss: 0.1459, Val Loss: 0.2422\n",
            "Epoch 16/50, Loss: 0.1397, Val Loss: 0.2388\n",
            "Epoch 17/50, Loss: 0.1040, Val Loss: 0.2404\n",
            "Epoch 18/50, Loss: 0.0924, Val Loss: 0.2493\n",
            "Epoch 19/50, Loss: 0.0848, Val Loss: 0.2461\n",
            "Epoch 20/50, Loss: 0.0810, Val Loss: 0.2582\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.33%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 0.6270, Val Loss: 0.3613\n",
            "Epoch 2/100, Loss: 0.3875, Val Loss: 0.3003\n",
            "Epoch 3/100, Loss: 0.3231, Val Loss: 0.2661\n",
            "Epoch 4/100, Loss: 0.2893, Val Loss: 0.2521\n",
            "Epoch 5/100, Loss: 0.2632, Val Loss: 0.2426\n",
            "Epoch 6/100, Loss: 0.2397, Val Loss: 0.2321\n",
            "Epoch 7/100, Loss: 0.2253, Val Loss: 0.2203\n",
            "Epoch 8/100, Loss: 0.2138, Val Loss: 0.2245\n",
            "Epoch 9/100, Loss: 0.1989, Val Loss: 0.2306\n",
            "Epoch 10/100, Loss: 0.1871, Val Loss: 0.2214\n",
            "Epoch 11/100, Loss: 0.1782, Val Loss: 0.2387\n",
            "Epoch 12/100, Loss: 0.1684, Val Loss: 0.2279\n",
            "Epoch 13/100, Loss: 0.1573, Val Loss: 0.2324\n",
            "Epoch 14/100, Loss: 0.1218, Val Loss: 0.2195\n",
            "Epoch 15/100, Loss: 0.1089, Val Loss: 0.2317\n",
            "Epoch 16/100, Loss: 0.1032, Val Loss: 0.2456\n",
            "Epoch 17/100, Loss: 0.0969, Val Loss: 0.2500\n",
            "Epoch 18/100, Loss: 0.0932, Val Loss: 0.2530\n",
            "Epoch 19/100, Loss: 0.0864, Val Loss: 0.2472\n",
            "Epoch 20/100, Loss: 0.0828, Val Loss: 0.2602\n",
            "Epoch 21/100, Loss: 0.0670, Val Loss: 0.2625\n",
            "Epoch 22/100, Loss: 0.0588, Val Loss: 0.2714\n",
            "Epoch 23/100, Loss: 0.0558, Val Loss: 0.2770\n",
            "Epoch 24/100, Loss: 0.0550, Val Loss: 0.2770\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.79%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 0.6198, Val Loss: 0.3670\n",
            "Epoch 2/250, Loss: 0.3809, Val Loss: 0.2920\n",
            "Epoch 3/250, Loss: 0.3158, Val Loss: 0.2652\n",
            "Epoch 4/250, Loss: 0.2837, Val Loss: 0.2445\n",
            "Epoch 5/250, Loss: 0.2562, Val Loss: 0.2366\n",
            "Epoch 6/250, Loss: 0.2407, Val Loss: 0.2351\n",
            "Epoch 7/250, Loss: 0.2221, Val Loss: 0.2255\n",
            "Epoch 8/250, Loss: 0.2059, Val Loss: 0.2338\n",
            "Epoch 9/250, Loss: 0.1939, Val Loss: 0.2304\n",
            "Epoch 10/250, Loss: 0.1815, Val Loss: 0.2233\n",
            "Epoch 11/250, Loss: 0.1745, Val Loss: 0.2326\n",
            "Epoch 12/250, Loss: 0.1641, Val Loss: 0.2249\n",
            "Epoch 13/250, Loss: 0.1529, Val Loss: 0.2276\n",
            "Epoch 14/250, Loss: 0.1448, Val Loss: 0.2354\n",
            "Epoch 15/250, Loss: 0.1413, Val Loss: 0.2325\n",
            "Epoch 16/250, Loss: 0.1321, Val Loss: 0.2354\n",
            "Epoch 17/250, Loss: 0.0980, Val Loss: 0.2318\n",
            "Epoch 18/250, Loss: 0.0867, Val Loss: 0.2518\n",
            "Epoch 19/250, Loss: 0.0800, Val Loss: 0.2575\n",
            "Epoch 20/250, Loss: 0.0741, Val Loss: 0.2725\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.05%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 0.6207, Val Loss: 0.3575\n",
            "Epoch 2/350, Loss: 0.3836, Val Loss: 0.2920\n",
            "Epoch 3/350, Loss: 0.3251, Val Loss: 0.2793\n",
            "Epoch 4/350, Loss: 0.2875, Val Loss: 0.2461\n",
            "Epoch 5/350, Loss: 0.2624, Val Loss: 0.2278\n",
            "Epoch 6/350, Loss: 0.2432, Val Loss: 0.2304\n",
            "Epoch 7/350, Loss: 0.2230, Val Loss: 0.2233\n",
            "Epoch 8/350, Loss: 0.2146, Val Loss: 0.2254\n",
            "Epoch 9/350, Loss: 0.1973, Val Loss: 0.2177\n",
            "Epoch 10/350, Loss: 0.1869, Val Loss: 0.2154\n",
            "Epoch 11/350, Loss: 0.1802, Val Loss: 0.2298\n",
            "Epoch 12/350, Loss: 0.1714, Val Loss: 0.2170\n",
            "Epoch 13/350, Loss: 0.1633, Val Loss: 0.2188\n",
            "Epoch 14/350, Loss: 0.1543, Val Loss: 0.2233\n",
            "Epoch 15/350, Loss: 0.1461, Val Loss: 0.2277\n",
            "Epoch 16/350, Loss: 0.1385, Val Loss: 0.2373\n",
            "Epoch 17/350, Loss: 0.1034, Val Loss: 0.2312\n",
            "Epoch 18/350, Loss: 0.0925, Val Loss: 0.2472\n",
            "Epoch 19/350, Loss: 0.0889, Val Loss: 0.2563\n",
            "Epoch 20/350, Loss: 0.0818, Val Loss: 0.2680\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.16%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/5, Loss: 1.3676, Val Loss: 0.4339\n",
            "Epoch 2/5, Loss: 0.5842, Val Loss: 0.4082\n",
            "Epoch 3/5, Loss: 0.5547, Val Loss: 0.4284\n",
            "Epoch 4/5, Loss: 0.5429, Val Loss: 0.5004\n",
            "Epoch 5/5, Loss: 0.5364, Val Loss: 0.4276\n",
            "Accuracy: 84.47%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 2.7302, Val Loss: 0.5697\n"
          ]
        }
      ]
    }
  ]
}